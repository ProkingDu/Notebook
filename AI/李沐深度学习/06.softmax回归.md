## Softmax回归

softmax虽然名为回归的，但是实际上他解决的是分类的问题。

### 分类VS回归

- 回归：对连续值进行预测。
- 分类：对离散值进行类别预测。

两个经典的数据集：

- MNIST 手写数字识别（10类）
- ImageNet: 自然物体分类（1000类）
### 从回归到多类分类
![小杜的个人图床](http://src.xiaodu0.com/2024/10/09/eb36d16afac02333b8a34204ef373437.png)

如上图所示，根据之前的线性回归可以总结出，线性回归的主要；特点：
1. 单连续值的输出
2. 输出在自然区间R
3. 跟真实值的区别作为损失

而分类问题与回归问题最显著的区别是，二者的不同， 上图左部分是回归问题的输出，他是一个单连续数值。
在右半部分是分类问题。

特点：
1. **通常为多个输出**
2. 输出是样本预测为第i类的置信度。


由于分类不是一个数值，通常是一个字符串或者其他的特征标志，所以我们要将分类数据转为数值数据。

通常使用 **one-hot编码（也称独热编码）** 来实现将分类数据转为数值数据。

如下图所示：
![小杜的个人图床](http://src.xiaodu0.com/2024/10/09/78a10e05a0a541bc30f5ff9b5a06354c.png)

将一组类别转为二进制向量，其中只有一位为1，其他的都是0，也就是当预测到的y正好是第二个标签的时候，这个y的值为1，否则为0。

也就是说，对于一组离散的类别，将其通过一组二进制向量表示，如果输入的样本属于某个类别，则这个类别的值为1，其他类别的值都为0。

在确定了分类转为数值的方法之后，就可以通过回归思想中基本的均方误差来进行训练。

假设已经训练好一个模型，我们选取每个分类置信度最大的分类作为预测结果，将分类编码对应此分类的二进制向量值设为1，其他值为0.

又由于只有一一位有效编码，并且以最大值作为预测的分类，而为了提高预测置信水平，我们希望预测到的分类的置信度与其他分类的置信度尽可能的大。

如下图所示：
![小杜的个人图床](http://src.xiaodu0.com/2024/10/09/e8a48bfd751c3cdd1f500d2616abeea6.png)

我们希望真实分类的置信度与其他的分类置信度的差值要大于一个阈值来表明这个分类足够可信。



同时还有一个问题，通常情况下针对一个样本的预测，我们预想的输出是一个概率分布，他需要满足非负且和为1,。

这时候通过传统的线性回归模型是无法满足的，所以我们需要引入softmax函数。

通过softmax函数来对输出类别的置信度归一化，将其转为和为1的概率分布。

softmax函数的定义如下：

![小杜的个人图床](http://src.xiaodu0.com/2024/10/09/4f433267928b57ecc66792ef9b84df42.png)

也就是说，softmax函数将每个输出值取自然对数的底数e的指数值，然后将其除以所有输出值取自然对数的底数e的指数值的和，从而将输出值归一化到0-1之间。

每一个分类置信度归一化之后的值即为预测的概率，概率值最大的那个即为预测的分类。

由于置信度最大的那个分类他的softmax回归之后输出的值也是最大的，所以这个分类就是预测的分类。

而这里的损失就可以简化为真实的概率与预测的概率之间的差值。

### softmax与交叉熵损失

交叉熵损失是分类问题中最常用的损失之一。

交叉熵通常用于衡量两个概率的区别，他的定义如下：

![小杜的个人图床](http://src.xiaodu0.com/2024/10/09/74cc5fced93385afcb7c1d8508e3ad84.png)在softmax回归中，我们通过交叉熵函数来衡量真实概率与预测概率之间的区别作为损失函数。

也就是将上述公式的p、q改为预测值和真实值，表达如下：
$$
l(y,\hat{y}) = -\sum_{i}^ny\ln(\hat{y})
$$
就是将每个预测的类别的概率的自然对数乘以真实的类别标签向量之后累加求和再取负数。

由于真实的类别向量是通过独热编码表示的，也就是不是真正分类的类别值为0，所以在交叉熵展开后这些分类的累加项都是0，而正确的分类的值为1，最后可以将交叉熵公式简化为：
$$
l(y,\hat{y}) = -\ln(\hat{y}_{y})
$$

所以最后也可以得出结论，在分类问题中，我们不关心非正确类的预测值，只需要关注正确类的置信度有多大。

举个例子：

假设有一个三分类问题，模型的预测概率为 y′=[0.2,0.7,0.1]y′=[0.2,0.7,0.1]，真实标签为 y=[0,1,0]y=[0,1,0]（表示第二类是正确类别）。那么多分类交叉熵损失为：

$$
L(y,y′)=−(0⋅log(0.2)+1⋅log(0.7)+0⋅log(0.1))=−log(0.7)≈0.357
$$

最后，关于softmax回归的梯度：
![小杜的个人图床](http://src.xiaodu0.com/2024/10/09/1d132671cd019417ccd55a83118c81dd.png)

实际上就是预测值和真实值之差。

也就是，在进行梯度下降的时候，不断减去之前的梯度即减去预测值和真实值之间的差来提高预测准确率。



/quick