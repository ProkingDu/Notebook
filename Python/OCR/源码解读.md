
## 整体思路

由于直接分析源代码片段有些吃力，先把项目整体框架和思路梳理一遍。

首先是`/train_code/`，这个目录是训练CTPN和RCNN模型代码文件所在的目录，由于OCR整体的实现是通过CTPN识别特征然后通过CRNN进行文字识别，所以首先进行的是CTPN的训练，在`train_ctpn/`中：

`train_code/train_ctpn/data/dataset.py` 用于从图片和标签目录中加载数据集，通过继承pytorch的Dataset类和重载`__init()__`方法和`__getItem()__`方法来读取目录中的图片数据并且返回图片内容的张量。
*`train_code/train_ctpn/train_data/train_img`* 此目录是训练集图片的储存目录
*`train_code/train_ctpn/train_data/train_label`* 此目录是训练集标签的储存目录

### 检测模块候选框生成

在之前的OCR整体模型中第一个步骤是通过VGG网络来提取特征，生成检测模块候选框，如下图所示：

![小杜的个人图床](http://src.xiaodu0.com/2024/05/12/31e55b276e8f7efda82ed89c7e0dec4c.png)

在具体实现上，由于样布数据的来源是图片，所以首先是对数据集中的图片进行读取和生成样本数据，在：*`train_code/train_ctpn/data/dataset.py`* 中定义的__getitem__方法即用于读取某个图片的样本数据，具体实现：
```python
if not os.path.isdir(datadir):  
    raise Exception('[ERROR] {} is not a directory'.format(datadir))  
if not os.path.isdir(labelsdir):  
    raise Exception('[ERROR] {} is not a directory'.format(labelsdir))  
  
self.datadir = datadir  
self.img_names = os.listdir(self.datadir)  
self.labelsdir = labelsdir
```

上述代码是*ICDARDataset*类的构造方法，在对象被实例化时自动调用，其含义是将数据集目录下的所有图片文件路径构成一个类成员以便其他方法调用。

在getitem中：
```python
img_name = self.img_names[idx]  
img_path = os.path.join(self.datadir, img_name)  
# print(img_path)  
img = cv2.imread(img_path)  
#####for read error, use default image#####  
if img is None:  
    with open('error_imgs.txt','a') as f:  
        f.write('{}\n'.format(img_path))  
    img_name = 'img_2647.jpg'  
    img_path = os.path.join(self.datadir, img_name)  
    img = cv2.imread(img_path)
```

这段代码用于读取指定一张图片的通道数据，其中self.img_names即构造方法中生成的图片列表，然后获得图片的绝对路径，通过opencv读取图片像素点数据。

之后，有如下代码：
```python
h, w, c = img.shape  
rescale_fac = max(h, w) / 1600  
if rescale_fac>1.0:  
    h = int(h/rescale_fac)  
    w = int(w/rescale_fac)  
    img = cv2.resize(img,(w,h))  
  
gt_path = os.path.join(self.labelsdir, 'gt_'+img_name.split('.')[0]+'.txt')  
gtbox = self.parse_gtfile(gt_path,rescale_fac)
```

由于读取的图像数据需要通过VGG网络进行处理，而**实际上数据集中的图像大小很可能是不一致**的，因此要通过**规定一个指定的大小来对图像进行resize**，通常这个大小要匹配数据集中大多数图片的尺寸，例如上述代码将宽或者高度二者最大的一边除以1600来得到一个缩放因子，当缩放因此大于1时即宽||高大于1600，则将宽||高/缩放因此使长边为1600，另一边按比例缩小，之后通过cv2来缩放图片。

最后两行代码是读取图片对应的ground truth数据即数据集图片对应的label文件，然后对、对标签数据进行处理。


接着往下：
```python
# clip image  
if np.random.randint(2) == 1:  
    img = img[:, ::-1, :]  
    newx1 = w - gtbox[:, 2] - 1  
    newx2 = w - gtbox[:, 0] - 1  
    gtbox[:, 0] = newx1  
    gtbox[:, 2] = **newx2**
```

这段代码用于将图像进行翻转操作，同时将翻转后的图片中的ground truth重新更新，**翻转图像是为了在原有数据集的基础上进一步扩展样本**，**翻转图片是一种常用的数据增强方法，他可以泛化模型、减轻模型过度拟合的风险。**

对图片进行处理完成之后：
```python
[cls, regr], base_anchors = cal_rpn((h, w), (int(h / 16), int(w / 16)), 16, gtbox)
```

通过传递处理后的图像宽高和特征图的宽高以及特征图的缩放倍数和标签的盒子作为参数，调用cal_rpn 方法来生成候选框。

在ctpn_utils中：
```python
def cal_rpn(imgsize, featuresize, scale, gtboxes):
	imgh, imgw = imgsize  
  
	# gen base anchor  
	base_anchor = gen_anchor(featuresize, scale)
```
首先通过imagesize得到图像的款到，即在dataset中传入的(h,w)，然后调用gen_anchor()来生成候选框，传递的参数是特征图的尺寸和缩放倍数。
**由于使用VGG16进行计算，其中含有五个池化层，其中第一个池化层是通过原始图像得到特征图，后面四个池化层对特征图进行四次池化，每次池化使特征度的深度扩大一倍，并且在二维上缩小一倍的尺寸，因此这里的缩放倍数是常量16。**
同时，传递的特征图尺寸应当是第一次池化得到的特征图尺寸即和原图一致的尺寸。

在*gen_anchor()*中：
```python
def gen_anchor(featuresize, scale):  
    """  
        gen base anchor from feature map [HXW][9][4]        reshape  [HXW][9][4] to [HXWX9][4]    """    
    heights = [11, 16, 23, 33, 48, 68, 97, 139, 198, 283]  
    widths = [16, 16, 16, 16, 16, 16, 16, 16, 16, 16]  
  
    # gen k=9 anchor size (h,w)  
    heights = np.array(heights).reshape(len(heights), 1)  
    widths = np.array(widths).reshape(len(widths), 1)  
  
    base_anchor = np.array([0, 0, 15, 15])  
    # center x,y  
    xt = (base_anchor[0] + base_anchor[2]) * 0.5  
    yt = (base_anchor[1] + base_anchor[3]) * 0.5  
  
    # x1 y1 x2 y2  
    x1 = xt - widths * 0.5  
    y1 = yt - heights * 0.5  
    x2 = xt + widths * 0.5  
    y2 = yt + heights * 0.5  
    base_anchor = np.hstack((x1, y1, x2, y2))  
  
    h, w = featuresize  
    shift_x = np.arange(0, w) * scale  
    shift_y = np.arange(0, h) * scale  
    # apply shift  
    anchor = []  
    for i in shift_y:  
        for j in shift_x:  
            anchor.append(base_anchor + [j, i, j, i])  
    return np.array(anchor).reshape((-1, 4))
```

首先定义了候选框的宽度和高度，由于池化后的特征图每个像素点对应原图一个特征框大小，因此要对每个特征图的像素点找到原图的特征框，也称为候选锚框，这里的heights和widths即为特征框的宽度和高度组成的数组，他们是一一对应组合的，例如11,16组成高度为11宽度为16的特征框。
在CTPN中，约定宽度是相同的，通过不同的高度来确定候选框的实际范围。

然后，构建ndarray数组，将一维的列表转为二维数组，其中每行表示候选框的宽度或者高度。

接下来：
```python
# center x,y  
    xt = (base_anchor[0] + base_anchor[2]) * 0.5  
    yt = (base_anchor[1] + base_anchor[3]) * 0.5  
  
    # x1 y1 x2 y2  
    x1 = xt - widths * 0.5  
    y1 = yt - heights * 0.5  
    x2 = xt + widths * 0.5  
    y2 = yt + heights * 0.5  
    base_anchor = np.hstack((x1, y1, x2, y2))  
```

这段代码计算计算候选框的中心点坐标和候选框的左上角和右下角坐标，通过这两个坐标即可得到整个候选框的区域。

之后：
```python
h, w = featuresize  
shift_x = np.arange(0, w) * scale  
shift_y = np.arange(0, h) * scale  
# apply shift  
anchor = []  
for i in shift_y:  
    for j in shift_x:  
        anchor.append(base_anchor + [j, i, j, i])  
return np.array(anchor).reshape((-1, 4))
```

将特征图尺寸拆包为特征图的宽度，再乘以缩放倍数即16还原为原始图像上的候选框尺寸，之后将所有的候选框组合在一起返回。

回到调用处：
```python
# calculate iou  
overlaps = cal_overlaps(base_anchor, gtboxes)
```

由于上一步产生的候选框是通过特征图的像素点来的，他们是覆盖整个图像的，而实际上对候选框识别不需要这么多候选框，因此要对其进行过滤，即选择感兴趣的区域（IOU）。

实际上真正需要的候选框是ground truth附近的，即与数据集中的label中定义的位置的重合度高的候选框。

最后得到的候选框即检测文本所需要的候选框。
## 检测文本框的函数 get_det_boxes

代码：
```python
def get_det_boxes(image,display = True, expand = True):  
    image = resize(image, height=height)   # 重设图像尺寸  
    image_r = image.copy()  # 这里的image是原始的image构造为np.array，copy是复制一个数组  
    image_c = image.copy()  # 同上  
    h, w = image.shape[:2]  
    image = image.astype(np.float32) - config.IMAGE_MEAN   # 转换数组中的数据类型为float32 并且与config.MEAN即归一化的RGB均值相减。  
    image = torch.from_numpy(image.transpose(2, 0, 1)).unsqueeze(0).float()  
  
    with torch.no_grad():  
        image = image.to(device)  
        cls, regr = model(image)  
        cls_prob = F.softmax(cls, dim=-1).cpu().numpy()  
        regr = regr.cpu().numpy()  
        anchor = gen_anchor((int(h / 16), int(w / 16)), 16)  
        bbox = bbox_transfor_inv(anchor, regr)  
        bbox = clip_box(bbox, [h, w])  
        # print(bbox.shape)  
  
        fg = np.where(cls_prob[0, :, 1] > prob_thresh)[0]  
        # print(np.max(cls_prob[0, :, 1]))  
        select_anchor = bbox[fg, :]  
        select_score = cls_prob[0, fg, 1]  
        select_anchor = select_anchor.astype(np.int32)  
        # print(select_anchor.shape)  
        keep_index = filter_bbox(select_anchor, 16)  
  
        # nms  
        select_anchor = select_anchor[keep_index]  
        select_score = select_score[keep_index]  
        select_score = np.reshape(select_score, (select_score.shape[0], 1))  
        nmsbox = np.hstack((select_anchor, select_score))  
        keep = nms(nmsbox, 0.3)  
        # print(keep)  
        select_anchor = select_anchor[keep]  
        select_score = select_score[keep]  
  
        # text line-  
        textConn = TextProposalConnectorOriented()  
        text = textConn.get_text_lines(select_anchor, select_score, [h, w])  
  
        # expand text  
        if expand:  
            for idx in range(len(text)):  
                text[idx][0] = max(text[idx][0] - 10, 0)  
                text[idx][2] = min(text[idx][2] + 10, w - 1)  
                text[idx][4] = max(text[idx][4] - 10, 0)  
                text[idx][6] = min(text[idx][6] + 10, w - 1)  
  
  
        # print(text)  
        if display:  
            blank = np.zeros(image_c.shape,dtype=np.uint8)  
            for box in select_anchor:  
                pt1 = (box[0], box[1])  
                pt2 = (box[2], box[3])  
                blank = cv2.rectangle(blank, pt1, pt2, (50, 0, 0), -1)  
            image_c = image_c+blank  
            image_c[image_c>255] = 255  
            for i in text:  
                s = str(round(i[-1] * 100, 2)) + '%'  
                i = [int(j) for j in i]  
                cv2.line(image_c, (i[0], i[1]), (i[2], i[3]), (0, 0, 255), 2)  
                cv2.line(image_c, (i[0], i[1]), (i[4], i[5]), (0, 0, 255), 2)  
                cv2.line(image_c, (i[6], i[7]), (i[2], i[3]), (0, 0, 255), 2)  
                cv2.line(image_c, (i[4], i[5]), (i[6], i[7]), (0, 0, 255), 2)  
                cv2.putText(image_c, s, (i[0]+13, i[1]+13),  
                            cv2.FONT_HERSHEY_SIMPLEX,  
                            1,  
                            (255,0,0),  
                            2,  
                            cv2.LINE_AA)  
            # dis(image_c)  
        # print(text)        return text,image_c,image_r
```

他的引用在：
```python
def ocr(image):  
    # detect  
    text_recs, img_framed, image = get_det_boxes(image)  
    text_recs = sort_box(text_recs)  
    result = charRec(image, text_recs)  
    return result, img_framed
```

而OCR函数中的image参数来自：
```python
def single_pic_proc(image_file):  
    image = np.array(Image.open(image_file).convert('RGB'))  
    result, image_framed = ocr(image)  
    return result,image_framed
```

这里的image实参是通过PIL（Python Image Library）库的Image类产生的图像的矩阵，其中:
`open`方法用于打开图像
`convert`用于转换打开的图像的色彩模式为RGB。
最后通过`np.array()`将图像RGB通道构成的列表转为numpy的数组。
因此在`ocr()`和`get_det_boxes()`函数中得到的`image`的实参都是类似此的numpy数组。

关于np.array：
1. NumPy数组在创建时有一个**固定的大小**，不像Python列表（可以动态地增长）。**改变一个ndarray的大小将创建一个新的数组并删除原来的数组**。
2. **NumPy数组中的元素都被要求是相同的数据类型**，因此在内存中的大小也是相同的。例外的情况是：可以有(Python，包括NumPy)对象的数组，从而允许不同大小的元素的数组。
3. **NumPy数组便于对大量数据进行高级数学和其他类型的操作**。通常情况下，与使用Python的内置序列相比，这种操作的执行效率更高，代码更少。
4. NumPy（Numerical Python 的简称）提供了高效存储和操作密集数据缓存的接口。在某些方面，NumPy 数组与Python 内置的列表类型非常相似。但是随着数组在维度上变大，NumPy数组提供了更加高效的存储和数据操作。

```python
image = image.astype(np.float32) - config.IMAGE_MEAN   # 转换数组中的数据类型为float32 并且与config.MEAN即归一化的RGB均值相减。  
image = torch.from_numpy(image.transpose(2, 0, 1)).unsqueeze(0).float()
```

这两行代码是对图像数据进行基本处理比较重要的部分。

第一行是将原始的图像的RGB通道构成的ndarray数组的所有值转为float32之后与config.IMAGE_MEAN定义的数据集中所有图像的RGB通道的均值相减。

**这个步骤叫做均值归一化，是图像预处理中常用的一种技术，目的是将输入数据的中心移动到原点，即将数据的平均值调整为0。这有助于模型更快地收敛，因为它使数据分布更均匀，减少了模型训练过程中的偏差和方差。**

### 计算RGB均值

由于这个示例的代码已经计算过了这个均值，并且直接写入配置文件中作为常量，因此如果我改为其他的数据集可能就不适用这个模型，因此自己写了一段计算图像数据集的RGB通道均值的代码：
```python
import os  
import numpy as np  
from PIL import Image  
import torch  
# 导入必要的包  
  
mean = []  
channel_sum = torch.zeros(3)  
pixel_count = 0  
  
# 读取数据集：  
path = "../train_code/train_ctpn/train_data/train_img"   #数据集路径  
imgList = os.listdir("../train_code/train_ctpn/train_data/train_img")    # 图片名列表  
  
mean_tensor = torch.zeros(3)  
for imgName in imgList:  
    img_path = os.path.join(path,imgName)    # 文件路径  
    # 读取图像  
    try:  
        with Image.open(img_path) as img:  
            img = img.convert('RGB')  
    except IOError:  
        print(f"读取图像失败： {img_path}")  
        continue  
    item_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float()  
    # print(item_tensor.sum((1, 2)))  
    channel_sum = item_tensor.sum((1, 2))  
  
    # 更新总像素计数  
    pixel_count = item_tensor[0].numel()  # 一个通道的像素总数  
  
    # 计算单个均值  
    item_mean = channel_sum / pixel_count  
  
    # 加入均值张量  
    mean_tensor += item_mean  
  
# 计算累计均值  
mean = mean_tensor / len(imgList)  
print(mean)
```

也是由于之前没有太多接触过pytorch，这段代码废了我老鼻子劲了，折腾了几个小时才搞出来。

最大的坑是我把`Image.open().convert()`这个方法返回的值转为ndarray的格式搞错了，他返回的格式类似：
```
[[W],[H],C]
```

其中W指打开图像的宽度，H指打开的图像的高度，C为打开图像的通道，一开始我错误的认为C代表通道则他的内容应该是图像中RGB值组成的张量，但是怎么计算都得不到正确的结果，然后去查了一下：
1. **C (Channels)**：颜色通道数，对于标准的RGB图像，此值为3（红、绿、蓝）。对于灰度图像，此值为1。在一些特殊情况下，例如使用RGBA格式，可能会有第四个通道（Alpha，表示透明度）。
2. **H (Height)**：图像的高度，即图像垂直方向上的像素数。
3. **W (Width)**：图像的宽度，即图像水平方向上的像素数。

因此在计算RGB通道均值时，应通过H,W作为像素点RGB值的样本来累加，将C作为像素点总数的样本来统计。

整个代码通过os.listdir将数据集目录下的所有图片列为列表，并且遍历这个列表，对每个图像的相关参数进行计算。

1. 通过PIL的Image类打开图像，并且转为RGB格式，此时返回的值是一个文件句柄。
2. 将文件句柄转为ndarray，然后在转为pytorch张量
3. 对pytorch张量进行整理，使用`permute`对维度换位，使得张量的shape由`H,W,C`转为`C,H,W`
4. 转换张量类型，全部替换为float32
5. 计算channel_num即每个通道的和，通过tensor.sum计算垂直方向（H）和水平方向（W）所有的像素点RGB值的和，得到的是一个类似`tensor([R, G, B])`的值。
6. 计算pixel_count即图像累计像素点，将二者相除即得到当前图像的RGB均值。
7. 将当前图像RGB均值与累计的均值相加。
8. 最后累计均值除以数据集数量即得到最后的均值。

这里运行的结果是：
```python
tensor([135.6030, 127.0569, 116.6526])
```




### 数组重塑与转置

接着分析第二行代码，其中的比较复杂的部分是`transpose()`对数组的转置操作。
对于二维数组的转置比较容易理解，实质上就是行列变换，但是对于三维矩阵甚至高维矩阵就稍微复杂，例如图像像素点构成的ndarray就是个典型的高维数组。

先看个小栗子：
```python
arr = np.arange(1,13).reshape(2,2,3)  
print(arr,"\r\n转置\r\n",arr.transpose())
```

结果是：
```
[[[ 1  2  3]
  [ 4  5  6]]

 [[ 7  8  9]
  [10 11 12]]] 
转置
 [[[ 1  7]
  [ 4 10]]

 [[ 2  8]
  [ 5 11]]

 [[ 3  9]
  [ 6 12]]]
```

原先是由1-12构成的三维数组，其中**reshape中参数指定每一维度有多少元素**，例如上述的2,2,3指定第一维有两个元素，第二维有两个元素，第三维有三个元素。
在结果中体现为：一共有两个二维数组，每个二维数组中有两个一维数组，每个一维数组有三个元素。
**这里的维度是从高到低的，也就是说第一维是三维数组，第二维度是二维数组，以此类推。**
即：**第一个参数是最高位的子元素或者所子数组数量，最后一个参数是最低维即一维数组的元素个数。**
注意：**通过reshape更改数组维度必须保证前后的元素数量一致。**
计算公式：`p1*p2*p3*.....*pn = s`其中pn为reshape中的参数，s为原先数组的元素个数。

例如：随机构建从1到1000步长为10累计元素数量为100的五维数组：
```python
arr_100 = np.arange(1,1001,10).reshape(5,5,2,1,2)  
print(arr_100)
```

reshape的参数表示这个五维数组中有5个四维数组，每个四维数组中有5个三维数组，所以一共有四维数组个数\*三维数组个数即5\*5=25个三维数组。
**对于高维数组，计算其中含有低维数组个数的方式：`pn*p(n-1)*p(n-2)*......*p1`即当前维度的个数向前累乘到第一个参数。**

这段代码：
```python
image = torch.from_numpy(image.transpose(2, 0, 1)).unsqueeze(0).float()
```

其中的transpose起到的作用就是交换维度，默认情况下，使用像Pillow这样的图像库加载的图像数据数组通常具有形状（Height, Width, Channels），即（高，宽，通道）。然而，PyTorch在处理图像数据时，期望的输入形状通常是（Channels, Height, Width）。所以这里的 `image.transpose(2, 0, 1)` 就是将通道轴移动到最前，高和宽依次排列。
即：
- **2** 表示原始第三维（通道）现在应该是第一维。
- **0** 表示原始第一维（高）现在应该是第二维。
- **1** 表示原始第二维（宽）现在应该是第三维。



### 关于梯度与自动求导

代码中有一句：
```
with torch.no_grad():
```
