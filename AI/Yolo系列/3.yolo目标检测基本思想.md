
## 检测过程

Yolo对目标检测任务的整个过程可以大致分为三个主要步骤：
1. **接受原始图像**、**对图像进行缩放**
2. **经过神经网络计算**
3. **使用非极大值抑制（Non-Max Suppression）得到得到预测框**

**对图片进行缩放-->经过卷积网络-->非极大抑制**

Yolo检测的核心思想如下图所示：
![](https://i-blog.csdnimg.cn/blog_migrate/4e1afedc981a7c7c535502d016b942a3.jpeg)



首先，在对图像进行缩放处理后，经过卷积网络在[特征图](https://so.csdn.net/so/search?q=%E7%89%B9%E5%BE%81%E5%9B%BE&spm=1001.2101.3001.7020)上划分S×S的网格，通过网格的划分得到边界框（bounding box）和置信度得分（confidence）以及类别的概率图（class probability map），结合两者得到最终检测结果。

![](https://i-blog.csdnimg.cn/blog_migrate/6734d01f8dcb1afc76f8021727cdf6e4.png)

从图像的特征图出发，得到物体检测出的属性（attributes），即边界框的坐标（box co-ordinates）、目标性得分（objectness score）、分类的得分。

![](https://i-blog.csdnimg.cn/blog_migrate/e90ec7328332c510a964e7d1256190fa.png)

这里的计算方式是结合边界框坐标、物体检测得分和分类得分乘以预测到边界框的数量来计算最终的检测得分。
其中Class scores从数据集中包含的类别中来，例如VOC数据集这里就是20个得分，COCO数据集这里就是80个得分。


![](https://i-blog.csdnimg.cn/blog_migrate/1296b1dec0bcb85627ba907c78d875ff.png)

尺度划分越细，有助于小目标的检测

[【目标检测】FPN(Feature Pyramid Network) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/62604038 "【目标检测】FPN(Feature Pyramid Network) - 知乎 (zhihu.com)")


## 锚框机制

![](https://i-blog.csdnimg.cn/blog_migrate/12b973c2d4570dcfc34872f1cd56b093.png)

锚框（Anchor Box）是预定义的一组矩形框，算法通过这些框来预测图像中物体的位置和大小。

YOLOv5 在训练开始时，会为每个检测层预定义一组锚框。这些锚框是根据数据集中特定类别物体的尺寸和形状分布，通过 **`k-means` 聚类方法**生成的。锚框有不同的宽高比（aspect ratio），可以有效捕捉物体的各种尺寸。对于每个检测层（通常有三个检测层，用于检测不同尺度的物体），锚框的大小也不同。通常，底层用于检测大物体，而高层则用于检测较小的物体。

在训练过程中，YOLOv5 会将真实的物体框与锚框进行匹配。如果锚框与某个真实框的 **IoU**（Intersection over Union, 交并比）大于某个阈值（通常为0.5），该锚框将被视为用于预测该物体的候选框。训练时，YOLOv5 会基于这些匹配的锚框来预测偏移量，使锚框与真实的物体框更接近。

对于匹配的锚框，YOLOv5 并不会直接输出物体的绝对边界框位置，而是预测一个相对于锚框的偏移量。具体来说，预测的输出包括以下几项：

- **位置偏移（tx, ty）：** 表示物体中心相对于锚框左上角的偏移量。
- **宽高缩放系数（tw, th）：** 表示物体宽度和高度相对于锚框的缩放比。
- **置信度得分（confidence）：** 代表该锚框中是否有物体。
- **类别得分（class probability）：** 代表该物体属于哪个类别。

通过这些偏移量，YOLOv5 可以将锚框调整为更加准确的边界框。

YOLOv5 使用**多尺度检测策略**，即在不同的网络层分别进行检测。每一层都有不同的锚框集合，适应不同大小的物体。这种多尺度机制使得 YOLOv5 可以同时检测到小物体和大物体。通常情况下，YOLOv5 的模型会有三个不同尺度的检测头，每个检测头使用不同尺寸的特征图和对应的锚框。

总的来说，锚框就是预定义的一组矩形框，在进行目标检测时将真实物体与锚框匹配，通过预定义的锚框来确定检测物体的**候选框**，然后基于这些候选框来**预测偏移量**，是锚框与物体更加接近。然后根据偏移量将候选框调整为更加准确的**边界框**。

在推理时，YOLOv5 会对每个锚框生成多个预测框，最终**可能会有多个框围绕同一个物体**。为了消除重复检测的框，YOLOv5 会使用**非极大值抑制（NMS）** 来筛选出置信度最高的预测框，并且去掉重叠框（**IoU 超过一定阈值的框**）。

每个预测框的类别置信度得分（class confidence score）计算如下：

**class confidence score = box confidence score × conditional class probability**
分类置信度 = 边界框置信度x条件类别概率。

![](https://i-blog.csdnimg.cn/blog_migrate/23dd91aaf880d0f4d42a31a40f721917.png)

它测量**分类和定位（目标对象所在的位置）的置信度**。
其中：
Pr(Object）框内有物体-->1,否则0

![Pr(Class_{i}|Object)](https://latex.csdn.net/eq?Pr%28Class_%7Bi%7D%7CObject%29)--->以有物体为条件，是哪一种类别的概率

## 非极大值抑制

![](https://i-blog.csdnimg.cn/blog_migrate/947e1844e4a202b742b9c59840769dda.png)

如上图所示，在神经网络对图像预测之后输出的结果可能包含多个边界框，这时候需要使用非极大值抑制来将保留其中置信度水平最高的边界框，这也是yolo三个主要步骤中的后处理阶段的任务。

非极大值抑制的基本步骤：

- **按置信度排序**：首先根据每个检测框的置信度（confidence score）进行排序。置信度高的框具有更大的保留优先级。
- **选择最高置信度框**：从排序后的框中选择置信度最高的框作为保留框。
- **计算重叠区域（IoU, Intersection over Union）**：
    计算当前框与其他框之间的交并比（IoU）。IoU 是指两个框的交集面积与并集面积的比值。其值在 0 到 1 之间，越接近 1 表示两个框越重叠。
- **抑制重叠过大的框**：对于 IoU 值大于某个阈值的框（通常设定为 0.5 或 0.7），认为它们与当前的保留框代表的是同一个目标，因此将其抑制（即删除）。
- **重复步骤 2 到 4**：继续选择剩余框中置信度最高的框，重复执行抑制步骤，直到处理完所有候选框。


## 损失函数（Loss function）

损失函数包括：
**classification loss，分类损失**
**localization loss， 定位损失（预测边界框与GT之间的误差）**
**confidence loss，置信度损失（框的目标性，objectness of the box）**

总的损失函数：
**classification loss+ localization loss + confidence loss**