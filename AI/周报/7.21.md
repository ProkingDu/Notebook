# 关于Prompt设计

## 什么是Prompt

在人工智能领域，Prompt 是一个非常重要的概念。它是指 **一段给定的文本或语句，用于启动和引导机器学习模型生成特定类型、主题或格式的输出。** 在最近几年中，随着深度学习技术不断发展和进步，在自然语言处理、图像识别等领域中广泛使用了 Prompt 技术。

**Prompt可以理解为启动机器学习模型的方式，他是一段文本或者语句，用于引导模型产生特定类型、格式或者主题的输出。**

在自然语言处理领域中，Prompt 通常由一个问题或任务描述组成，例如“给我写一篇有关人工智能的文章”、“翻译这个英文句子到法语”等等。在图像识别领域中，Prompt 则可以是一个图片描述、标签或分类信息。

与传统的监督式训练不同，**在使用 Prompt 进行训练时，并不需要对每个样本都进行标注，而是通过给定的 Prompt 来引导模型生成相应的输出。** 这种方式可以大大减少数据标注的工作量，并且能够更好地控制模型生成结果的质量和多样性。


## Prompt的应用场景

依据大模型的推理能力，Prompt具有多种适用的场景，最具有典型的是在大语言模型中的自然语言处理能力。

其中大概可以包括：
1. 文本生成
2. 问答系统
3. 信息提取
4. 文本翻译
5. ......


对于具有图像处理能力的大模型，其通常可以通过设计合理的Prompt来进行适应的图像处理任务，例如：
1. 图像生成
2. 图像分类
3. 图像解释
4. 图像优化
5. ......


无论是单纯处理文本的大语言模型，还是具有图像处理能力的大模型，设计Prompt都是具有一般性的方法，接下来将对Prompt设计的要点进行陈述。

## Co-Star框架
在Prompt设计中，符合惯例的一个较好的设计原则是通过**CO-Star框架**来搭建Prompt架构。

CO-STAR 框架是一种可以方便用于设计 prompt 结构的模板，这是新加坡政府科技局的数据科学与 AI 团队的创意成果。该模板考虑了会影响 LLM 响应的有效性和相关性的方方面面，从而有助于得到更优的响应。

![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/d1716010e2b6ba1710403df06647751b.png)

如上图所示，Co-Star框架包含六个方面：
1. 上下文（Context）
2. 目标（Objective）
3. 风格（Style）
4. 语气（Tone）
5. 受众（Audience）
6. 响应（Response）

### Context

在Prompt设计中，上下文应当提供任务有关的背景信息，这有助于LLM理解正在讨论的具体场景，从而确保其响应是相关的。


### Objective

通过Objective定义你希望 LLM 执行的任务。明晰目标有助于 LLM 将自己响应重点放在完成具体任务上。

通常我们在设计Prompt的时候就已经大概明确了目标，例如需要大模型生成一段关于个人介绍的文本，亦或者是对某个图像进行解释等等。

### Style

指定你希望 LLM 使用的写作风格。这可能是一位具体名人的写作风格，也可以是某种职业专家（比如商业分析师或 CEO）的风格。这能引导 LLM 使用符合你需求的方式和词语给出响应。

### Tone

设定响应的态度。这能确保 LLM 的响应符合所需的情感或情绪上下文，比如正式、幽默、善解人意等。

### Audience

确定响应的目标受众。针对具体受众（比如领域专家、初学者、孩童）定制 LLM 的响
应，确保其在你所需的上下文中是适当的和可被理解的。


### Response

提供响应的格式。这能确保 LLM 输出你的下游任务所需的格式，比如列表、JSON、专业报告等。对于大多数通过程序化方法将 LLM 响应用于下游任务的 LLM 应用而言，理想的输出格式是 JSON。

### Example

根据以上Co-star框架的六个要素，尝试设计一个简答的Prompt和使用Co-star框架的Prompt来对比GPT-4的回答。

首先是简单的Prompt：
```
请帮我设计一份用户登录的前端页面。
```

GPT-4的回复是这样一份简单的页面：

![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/0beb4160eb2c480fa4eea18d2be839ca.png)

应用了Co-star框架的Prompt：
```
我在一家网络传媒公司做前端设计的工作，现在我的上司为我分配了一个设计为公司官网提供用户注册功能的页面，同时需要在页面中切换登录和注册两个表单。并且这个页面需要符合公司形象和具有行业元素。
# Context
请你帮我设计一份这样的一份网页，提供给我源代码，这个页面旨在增加用户的交互性和体验感，同时丰富公司官网的功能。
# Objective
遵循类似的前端设计标准，保证网页设计的规范性以及布局的交互性，网页整体要具有高级感，需要具有一定的装饰元素。
# Style
内容是具有吸引力的、能够产生情感共鸣的。
#Tone
使用这个页面的客户群体是访问我们公司官网的人群，通常是同类行业从业者和潜在客户，或者是从事自媒体相关的工作人员，以及对网站进行备案审核的审核人员。
# Audience
请直接给我这样的一份网页源代码，代码应当符合相关技术上的代码规范，使用原生的前端技术，不限长度，在可能的前提下，多做出回复，产生尽量多的页面内容。
# Response
```

应用了Co-Star框架后输出的页面：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/7a87b55905bbb19b92822c9671f54285.png)

相比原来的页面，这个页面加入了网页背景，并且在页面载入时添加了过渡效果。

实际上上面的Prompt仍然不够完善，针对前端设计这类复杂的需求，还应当包括如下具体的方面以产生预期的成效：
1. 页面具体包含的内容，例如包含表单，表单元素包含的内容
2. 页面的布局样式，例如是水平布局还是垂直布局，是双栏布局还是单栏布局。
3. 页面的配色
4. 使用的技术栈等

再次优化上面的Prompt：
```
我在一家网络传媒公司做前端设计的工作，现在我的上司为我分配了一个设计为公司官网提供用户注册功能的页面，同时需要在页面中切换登录和注册两个表单。并且这个页面需要符合公司形象和具有行业元素。
# Context
请你帮我设计一份这样的一份网页，提供给我源代码，这个页面旨在增加用户的交互性和体验感，同时丰富公司官网的功能。
注册的表单包括：昵称、账号、邮箱、密码、联络地址、验证码
登录的表单包括：账号、密码、验证码
# Objective
遵循类似的前端设计标准，保证网页设计的规范性以及布局的交互性，网页整体要具有高级感，需要具有一定的装饰元素。整个页面应当采用双栏布局，即左右两块可选的表单可以切换和隐藏。
页面配色使用橘色系作为主色调。
# Style
内容是具有吸引力的、能够产生情感共鸣的，页面具有温暖的感觉。
#Tone
使用这个页面的客户群体是访问我们公司官网的人群，通常是同类行业从业者和潜在客户，或者是从事自媒体相关的工作人员，以及对网站进行备案审核的审核人员。
# Audience
请直接给我这样的一份网页源代码，代码应当符合相关技术上的代码规范，使用原生的前端技术，不限长度，在可能的前提下，多做出回复，产生尽量多的页面内容。
# Response
```

![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/42c03a1f9efe3e6a51823df6b0fb421a.png)

这次产生的页面相对之前的更加符合要求，像是这类具有明确输入的任务，Prompt设计的核心要点只有两个字：**详细。**

如果是开放式的任务呢，例如一个典型的文本生成任务，在随意设计Prompt的情况下：
```
帮我生成一篇博客文章，详细的介绍JavaScript中的promise与异步机制。
```
![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/df6cd37f3edbc8bf1d621f5c73062616.png)

这一Prompt生成的内容已经较为详细合理了，如果加上Co-Star框架约定的要素呢，据此查看GPT的输出：
```
我是一名个人开发者，我搭建了一个基于wordpress的个人博客，我习惯于在博客上发布我的学习笔记和感悟，现在我想尝试通过AI来编写一些技术文章用以充实博客内容和提高搜索引擎权重。
# Context

请你帮我生成一篇关于JavaScript中的Promise对象与异步机制的文章，在文章中详细介绍Promise和异步编程，并且给出具体的代码示例和应用场景。
# Objective

假设你是一名资深的个人开发者，以传授经验的角度生成这篇文章。
# Style

文章包含的情感以严谨为主，同时伴随生动有趣的描述介绍。
# Tone

文章的受众群体是热爱编程开发的学习者，他们大多是从搜索引擎或者各种渠道访问我的博客的。
# Audience

你生成的文章应当具有流畅的行文结构，标题分布要合理，在HTML网页中能够清晰的展示出来，并且方便布局。
使用Markdown语法回答。
# Response
```

将其输出的结果不经整理直接复制到我的博客中发布：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/6f59917486c8a99372a2fc9affff30ad.png)

从右侧的目录可以看出，GPT输出了标准的Markdown格式内容，并且具有清晰的目录结构。

## 使用分隔符

对于给定文本数据，需要通过LLM对其进行分析，使用分割符来分割文本是一个极好的方法，通常我们使用特殊字符或者是XML标签对文本进行分割，将Prompt中的问题和数据分开。

例如：
```
我有一些比较长的对话数据，现在需要你将分析他们其中包含的情感，每个独立的文本通过XML标签包裹。
# Context

将以下整理好的文本分析并告诉我他们所表达的情绪，通过两个字的情绪形容词来描述。
# Objective

你只需要按顺序回答每个文本对应的情绪形容词即可。
# Style

响应的格式严格按照：<<情绪>>
# Response

文本数据如下：

<text>
胆子再大的市委书记也绝对不敢找反贪局双规纪委书记。这消息要是一传出来，省委书记直接就能把他给切喽，把他先规了再说。	黄一直混在媒体圈子，小桥老树人家好歹混到副处级，为人也比他低调多了，沉得下心用在文字上	小桥老树的三观正确、积极、正面，黄晓阳急功近利，认为自己文章写得好，当然也能当大官。
</text>
<text>
这年头玻璃心的人怎么这么多。29岁的人了，不管有你说的以上什么原因，对待女性及家人都应该有基本的底线和标准，为了自己不知道为什么的私欲，企图在QQ上就先以演戏的方式让别人厌恶，既不成熟也不尊重。调侃的就是这种人，活该	既然你说我是玻璃心，那你应该知道我那脆弱的心脏可承受不了你说我是玻璃心的事实，你看，已经碎得满地是渣渣了。礼尚往来是中华民族的传统美德，你既然送了我一颗晶莹的玻璃心，那我就回赠你几句话吧。首先，我对你对玻璃心的定义不认同，我不认为比你心里底线低的人都有颗玻璃心；其次，我不同意用自己讨厌的行为去恶心自己讨厌的人，那样，自己也就成了自己讨厌的那种人了是不？最后，我认为喜欢站在别人角度考虑问题的女孩最美了，而不是喜欢以自己的价值观为锚点，通过干掉自己不喜欢的人来实现世界和平的女孩。本来不想回的，只是评论区这么长，你还能看到并且评论我的话，还真不容易。	因为喜欢看大家如何调侃楼主的啊 然后偶尔看到个不一样的就顺手回复了 呵呵既没兴趣想干掉意见不同的你也没义务站在内心脆弱的角度去为他人着想 管我什么事嘛我们都是来看热闹的
</text>
<text>
可是我觉得他新女友才丑呢我不能算漂亮但是也不会差家里虽然不是大富但也是中层往上比他家好多了虽然不是本地人但在本地有俩套房产	其實是這樣的〜〜如果是我我肯定不跟妳分〜這些都是些什麼人啊渣男那麼多悲哀	表酱其实我就是不甘心很难过不甘心
</text>
```

（梯子掉了，用文心一言测试）![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/d3dee94e471555e928b611ef5a9666cd.png)

如果不对文本进行分割，而是使用换行的形式：
```
我有一些比较长的对话数据，现在需要你将分析他们其中包含的情感.
将以下整理好的文本分析并告诉我他们所表达的情绪，通过两个字的情绪形容词来描述。
你只需要按顺序回答每个文本对应的情绪形容词即可。
响应的格式严格按照：<<情绪>>


文本数据如下：
胆子再大的市委书记也绝对不敢找反贪局双规纪委书记。这消息要是一传出来，省委书记直接就能把他给切喽，把他先规了再说。	黄一直混在媒体圈子，小桥老树人家好歹混到副处级，为人也比他低调多了，沉得下心用在文字上	小桥老树的三观正确、积极、正面，黄晓阳急功近利，认为自己文章写得好，当然也能当大官。

这年头玻璃心的人怎么这么多。29岁的人了，不管有你说的以上什么原因，对待女性及家人都应该有基本的底线和标准，为了自己不知道为什么的私欲，企图在QQ上就先以演戏的方式让别人厌恶，既不成熟也不尊重。调侃的就是这种人，活该	既然你说我是玻璃心，那你应该知道我那脆弱的心脏可承受不了你说我是玻璃心的事实，你看，已经碎得满地是渣渣了。礼尚往来是中华民族的传统美德，你既然送了我一颗晶莹的玻璃心，那我就回赠你几句话吧。首先，我对你对玻璃心的定义不认同，我不认为比你心里底线低的人都有颗玻璃心；其次，我不同意用自己讨厌的行为去恶心自己讨厌的人，那样，自己也就成了自己讨厌的那种人了是不？最后，我认为喜欢站在别人角度考虑问题的女孩最美了，而不是喜欢以自己的价值观为锚点，通过干掉自己不喜欢的人来实现世界和平的女孩。本来不想回的，只是评论区这么长，你还能看到并且评论我的话，还真不容易。	因为喜欢看大家如何调侃楼主的啊 然后偶尔看到个不一样的就顺手回复了 呵呵既没兴趣想干掉意见不同的你也没义务站在内心脆弱的角度去为他人着想 管我什么事嘛我们都是来看热闹的

可是我觉得他新女友才丑呢我不能算漂亮但是也不会差家里虽然不是大富但也是中层往上比他家好多了虽然不是本地人但在本地有俩套房产	其實是這樣的〜〜如果是我我肯定不跟妳分〜這些都是些什麼人啊渣男那麼多悲哀	表酱其实我就是不甘心很难过不甘心
```

![小杜的个人图床](http://src.xiaodu0.com/2024/07/18/6a98fb09eb612e7c74e330e5121178ab.png)

LLM错误的将三段文本处理为了六段文本。


# 李沐深度学习-01.序言：关于深度学习


## AI地图

看如下一张图：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/c9610b43f51e094148f0db83b894510a.png)

这张图将AI划分为两个维度，在X轴包含的是三个不同的模式，在Y轴包含四个问题领域，其中包含符号学、概率模型、机器学习三个模式，以及感知、推理、知识、规划四个问题领域。

其中问题领域中的**感知指形如看到、听到环境中内容的能力。**
**推理是指基于感知的内容推理出相关的内容，从已有的资料迭代出新的资料。**
**知识则是通过感知和推理来形成自己的知识体系，相当于模型的训练，通过感知和推理数据集来形成一个特定的知识体系。**
**规划是基于已有的知识体系来对未来进行规划整理。**

在图中比较热门的三个方向，他们就是基于这是三个模式和四个问题领域。

**自然语言处理（NLP）：**

自然语言处理虽然取得了较大的进展，但是目前仍然处于简单的感知层次，相当于人脑对自然语言的感知和一系列处理，例如机器翻译就是自然语言处理中的一个重要应用。

 **（这里由于课程是21年的，放在2024年，根据我个人的理解，自然语言处理似乎已经突破了感知的问题领域，现在LLM完全能够基于给定的自然语言输入来推理出对应的输出，甚至在一些比较领先的大模型中，例如chatGPT，完全可以在一个会话中进行知识体系的构建。）**

NLP的发展经历了符号学、概率模型、机器学习三个模式阶段，因为语言本质就是符号，通过传统的符号学就可以对自然语言进行处理，而概率模型和机器学习，是将NLP的效率进一步提高了，或者说拓展的NLP的应用范围。

**计算机视觉：**

计算机视觉，相比自然语言处理，最直观的体现是图像和文字的差异，图像是由不同的像素点组成的，也就是不同的RGB或者HSV颜色数据，对图像的处理很显然通过符号学是难以达到的，颜色的表示，似乎不像文字那样有一个标准的符号。因此计算机视觉大部分是基于概率模型和机器学习实现的。

不过符号学并非完全不能应用于计算机视觉领域，例如最著名的Opencv这一开源框架是完全结合了这三个模式，如边缘检测（Canny边缘检测）、图像滤波（高斯滤波、均值滤波等）和形态学操作（膨胀、腐蚀等）这些图像处理技术中就采用了符号学的方案。

其中还有背景减法算法用于运动检测，卡尔曼滤波器用于目标跟踪，贝叶斯分类器用于模式识别等处理方案使用到了概率模型。

使用Opencv实现的目标检测、图像分类的应用，就采用了SVM（支持向量机）、KNN（K邻近算法）等典型机器学习算法。

**深度学习：**

在这里首先明确一点，关于人工智能、机器学习、深度学习这三个概念的关系。

首先，人工智能即AI（Artificial Intelligence）是一门研究和开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的科学。

人工智能是一门学科，他涉及的内容非常广泛，其中包含：专家系统、机器人学、自然语言处理、计算机视觉、机器学习等。

也就是说，机器学习是人工智能的一个子领域，他专注于研究和开发使计算机能够通过数据进行学习和改进性能的算法和技术。

而深度学习又是机器学习的一个子领域，相对于传统的机器学习，深度学习则利用多层神经网络（通常包含数十层甚至数百层）进行特征提取和模式识别。

深度学习中常见的模型有：卷积神经网络（CNN）、循环神经网络（RNN）、生成对抗网络（GAN）等
所以，这三个概念的关系是：
$$
DL \in ML \in AI
$$
然后，深度学习这个重要的领域，他也可以实现之前的自然语言处理和计算机视觉，以及其他的具体应用，例如语音和音频处理、医学和生物学、自动驾驶等。

## 深度学习的领域突破

深度学习的产生和发展对一些应用领域产生的极大的影响，通过深度学习技术，使这些领域产生的革命性的突破。

### 图片分类

在图片分类领域（计算机视觉的一个重要应用场景），深度学习带来了极大的收益，在深度学习对图像分类的任务中，数据集是一个重要的因素。

在 [image-net](http://image-net.org ) 这个网站中，包含了数百万张不同的图片数据集，这些图片可供任何人进行非盈利性的使用。

![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/b7d83b7c5b4ac71a690cd17a88225d7c.png)


上图表示图片分类领域中的错误率变化趋势，在深度学习开始发展时，图像分类任务的错误率极大的降低了。

### 物体检测和分割

[Mask R-CNN](https://github.com/matterport/Mask_RCNN) 是一个强大的物体检测和分割的开源项目，下图所示即为通过Mask R-CNN进行物体检测和分割的结果：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/12add84d765a809d30946f1561584183.png)

其中标注出了人和飞机的位置，这里再解释一下，物体检测即在图像中检测某个物体的位置，即他所处的图像区域，而分割则是判断图像中的一个像素点属于哪个物体。

### 样式迁移

[MXNet Gluon Style Transfer GitHub Repository](https://github.com/zhanghang1989/MXNet-Gluon-Style-Transfer) 是一个基于MXnet的样式迁移项目，样式迁移即通过给定两个输入图像，一张作为原始图像，一张作为参考图像，通过样式迁移可以将参考图像的风格样式迁移到原始图像，输出是处理后的原始图像。

这里我克隆了一下这个项目，并在本地部署运行，记录一下过程：
这里我首先从github上克隆项目：
```bash
git clone https://github.com/zhanghang1989/MXNet-Gluon-Style-Transfer.git
```

然后在conda中创建虚拟环境：
```bash
conda create --name Style-Transfer python=3.6
```

这里踩坑了，由于Style-Transfer是基于MXnet框架的，我起初创建的虚拟环境是3.11版本，但是在安装mxnet时一直报错，提示无法编译numpy，但是我已经安装好numpy，网上搜索了一大堆解决方案都尝试没有用，后面仔细看了一下报错信息，有一个依赖项的需要的Python版本是3.6，于是我将环境改成3.6版本，重新安装依赖，这时候安装上了mxnet，但是我想使用GPU版本的mxnet，查看我的CUDA版本：
```
nvidia-msi -q
```
CUDA版本是12.5，我尝试：
```python
pip install mxnet-cu125
```
但是无法找到包，我在阿里镜像站中也没有搜索到MX对应CUDA12.5版本的包。

这里我看到一篇文章安装的是1.10版本的，我试着硬着头皮安装：
```
pip install mxnet-cu110
```
这个版本的可以安装，但是和我的CUDA版本不匹配，很明显，无法执行脚本。

接着卸载MXnet和GPU版本的MXnet，重新安装。

只能通过CPU运算了，在Readme中给出了执行main.py的参数:
* `--content-image`: path to content image you want to stylize.  
* `--style-image`: path to style image (typically covered during the training).  
* `--model`: path to the pre-trained model to be used for stylizing the image.  
* `--output-image`: path for saving the output image.  
* `--content-size`: the content image size to test on.  
* `--cuda`: set it to 1 for running on GPU, 0 for CPU.

将--cuda参数设置为0即可只使用CPU进行运算，但是在执行的时候还是报出了一个错误：
```python
warnings.warn("load_params is deprecated. Please use load_parameters.")
```
这个就很好解决，打开报错文件，找到位置，发现是在调用`load_params`时报出的这个错误，这个函数只在main.py中有两个引用，根据报错信息，将其改为`load_parameters`即可。

完整运行一下：
```bash
python main.py eval --content-image images/content/venice-boat.jpg --style-image images/styles/candy.jpg --model models/21styles.params --content-size 1024 --cuda=0
```

等待几十秒，在项目根目录输出了结果图片，然后加上自定义参数--output-image自定义输出位置，注意这里项目使用的`fopen()`是直接创建文件，而没有做目录存在性检测，因此如果需要指定目录，需要先新建目录。
我在根目录下新建了output目录，然后运行：
```python
python main.py eval --content-image images/content/img.png --style-image images/styles/img.png --model models/21styles.params --content-size 1024 --cuda=0 --output-image images/output/a.jpg
```
这里的原图像是：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/f49715655da7f386e7124b9e90696304.png)

参照图像是：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/5def03890843169eebbbf0bcbbbc31d7.png)

输出的结果：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/2252a2e01177d94c60ee03f82cfff3bc.png)

这里就可以很直观地看出，样式迁移会将参考图片的画风和色调作为滤镜应用到原始图像。

### 文生图

文生图就是一个热门又经典的应用了，最典型的例子是OpenAI出品的Dell-E，通过给定文字描述产生相关的图片。

### 文字生成

除了计算机视觉方面，深度学习在NLP中也具有革命性的突破，最典型的就是各种用于语言文本生成的LLM。

### 无人驾驶

例如特斯拉的无人驾驶，还有最近讨论比较多的百度的那个叫什么的自动驾驶，这里就是将各种人工智能的应用场景综合在一起了，涉及到计算机视觉、机器人系统、专家系统等等。


## 案例

李沐老师在课程中提及到的是广告点击的案例，这里我发散了一下，根据广告点击案例的思路大概想了一下，如果通过机器学习来实现web防火墙的攻击预测的大概实现步骤。

首先，不管是广告推荐系统还是攻击预测系统，他们的本质实际上都是一类预测问题。

对于预测问题，通过深度学习来解决最关键的仍然是适当的数据集和训练好的模型。

针对一个攻击预测系统的实现，首先需要通过系统访问日志，将访问来源、访问目的、端口、连接类型、连接时间（如果有）、访问URI等信息搜集记录形成数据集，然后通过特征提取来对数据集中的数据提取特征，也就是上面提到的日志中包含的信息的特征，根据这些特征以及对应的神经网络来训练模型，例如在Encoder-Decoder架构就是将输入内容通过RNN或者LSTM来产生上下文向量，并且通过Encoder输出之后作为Decoder的输入。

模型训练完成之后，即可通过验证集来测试模型。例如随机挑选最近几十条访问日志，包含恶意用户和正常用户，通过模型预测他们是否存在攻击行为？在验证准确率较高的情况下，接入系统进行测试。

这里的预测流程是，针对每个访问的用户，获取其与训练集结构一致的字段，即访问来源、目的、端口、连接类型等，根据这些特征输入模型，通过模型预测其是恶意访问的概率来考虑是否放行。

# 李沐深度学习-02.下载课程源代码

目前先安装CPU版本的环境和示例程序。

动手学深度学习课程的配套源代码在：
[https://zh-v2.d2l.ai/d2l-zh.zip](https://zh-v2.d2l.ai/d2l-zh.zip)

如果这里下载的比较慢，可以从github项目下载：
[d2l-ai/d2l-pytorch-slides: Automatically Generated Notebook Slides (github.com)](https://github.com/d2l-ai/d2l-pytorch-slides)


先下载源代码，这会功夫把环境配置一下:
```cmd
conda create --name d2l python=3.8 pip
conda activate d2l
```

安装模块：
```python
pip install jupyter d2l torch torchvision
```

安装完成之后，运行jupyter：
```bash
jupyter notebook
```

启动成功之后自动在浏览器中打开：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/bd36d1463e32ddf905fc29aa37bfb7eb.png)

这里我已经把源代码解压完成并且切换到项目目录了。



# 李沐深度学习-03.数据操作+数据预处理（未完）
## 数据操作

### N维数组

N维数组是机器学习和神经网络中主要的数据结构。

如下图所示：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/c8b6b2e00d5257f752164e1387cf0d52.png)


其中我们把**一个单值称为标量。记作0-d**
一个**一维数组通常作为特征向量，记作1-d**
**二维数组即线性代数中常见的矩阵，我们称为特征矩阵，记作2-d**

![小杜的个人图床](http://src.xiaodu0.com/2024/07/19/3aa08d4f026d5b46f09490e29be46d1c.png)

如上图所示，对于更高维的数组，我们有比较常用的用法。

**一个3-d数组，也就是三维数组，通常用于储存RGB图片（宽x高x通道）**
其中列数就是图像的宽度，行数即为图像的高度，每个基本元素由一个一维数组表示其RGB通道的值。

**一个4-d数组，通常用于表示一个RGB图片批量，在深度学习中，我们称为一个Batch。**
实际上，这个4-d数组就是由多个3-d数组组成的，例如在读取数据集的时候一次性读取128章节图像，那么这个Batch的大小就是128。

**5-d数组典型是一个视频批量，由批量大小、时间、宽、高、通道组成。**

### 创建数组

创建数组需要给定三个参数：
1. 形状：例如3x4的矩阵
2. 数据类型：指定每个数组元素的数据类型
3. 元素值：指定每个数组元素的值，例如全为0或者随机数。

### 访问数组

![小杜的个人图床](http://src.xiaodu0.com/2024/07/20/de6f6d278a515267305872373500e23a.png)

如上图所示，访问数组元素的基本方法是：
```python
var[start_row:end_row:step_row,start_col:end_col:step_col]
```

例如，访问数组中N行M列的元素：
```
var[n,m]
```

访问第一行的元素：
```python
var[1,:]
```
访问第一列的元素：
```python
var[:,1]
```
访问某个区域的元素，这里以第二行到第三行，第二列到结尾为例：
```python
var[1:3,1:]
```
访问某个区域中以指定步长间隔的元素，这里以所有行列的元素，在行中步长为3，列中步长为2访问：
```python
var[::3,::2]
```
其中，步长为3指下一个访问的元素是当前索引+3，例如第一个访问的元素是0，则下一个是0+3=3，而不是中间间隔三个元素。

访问最后一个元素：
```python
var[-1]
```

## 数据操作实现

以上的关于数据操作的描述是基于Pytorch的张量，在数据操作的实现上，也采用Pytorch进行。
首先导入Pytorch：
```python
import torch
```

### 关于张量

在PyTorch中，张量（Tensor）是最基本的操作对象。它是一个多维数组，类似于NumPy的ndarray，但增加了对GPU加速计算的支持

#### 1.创建张量

可以从已有的数据中创建张量，通过`torch.tensor()`方法：
```python
list = [[1,2,3],[3,4,5]]  
tensor = torch.tensor(list)  
print(tensor)
"""
tensor([[1, 2, 3],
        [3, 4, 5]])
"""
```

**使用内置函数创建：**
torch内置较多的用于创建张量的函数，这里将他们以表格的形式列出。

| 函数名                                 | 参数                                                     | 说明                                    | 示例                                                                                                                                      |
| ----------------------------------- | ------------------------------------------------------ | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| torch.arange(start,end,step)        | start:开始<br>end：结尾<br>step:步长<br>                      | 创建从start到end-1的范围等步长的张量，元素数量不骨固定      | ``torch.arrange(0,12)``<br>``# 创建从0到11的12个元素的一维张量。``<br>``# tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])``                  |
| torch.linspace(start,end,step)      | start:开始<br>end:结尾<br>steps：数量                         | 创建从start到end-1数量为steps的张量，元素数量即为steps | # 从0到1的等间隔5个值 tensor = torch.linspace(0, 1, steps=5)<br>print(tensor)<br>“”“<br>tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])<br>“”“ |
| torch.eye(n)                        | n:指定创建nxn的矩阵                                           | 创建一个nxn的单位矩阵，对角元素为1，其余元素为0.           | # 3x3单位矩阵<br>tensor = torch.eye(3)<br>print(tensor)<br>"""<br>tensor([[1., 0., 0.],<br>[0., 1., 0.],<br>[0., 0., 1.]])<br>"""           |
| torch.full((row,col),vlaue)         | row:行数<br>col：列数<br>value：填充值<br><br>                  | 以给定值value填充生成row\*col大小的矩阵。           | <code># 所有元素都为7的2x3张量<br>tensor = torch.full((2, 3), 7)<br>print(tensor)<br>"""<br>tensor([[7, 7, 7],<br>[7, 7, 7]])<br>"""<br></code>  |
| torch.rand(row,col)                 | row:行<br>col：列                                         | 创建一个row行col列的随机元素值的张亮                 | # 创建一个随机张量 random_tensor = torch.rand(2, 3) print(random_tensor)                                                                        |
| torch.empty_like(tensor)            | tensor:参考张量                                            | 参考tensor张量创建一个未初始化的相同形状的张量            | data = torch.rand(3,3)  <br>data1 = torch.empty_like(data)  <br>print(data1)                                                            |
| torch.zeros_like(tensor)            | 同上                                                     | 参考tensor张量创建一个形状相同的，值全为0的张量。          | data = torch.full((5,6),0.01)<br>tensor = torch.zeros_like(data)<br>print(tensor)<br><br>                                               |
| torch.ones_like(tensor)             | 同上                                                     | 与zeros_like类似，创建全为1的张量。               | data = torch.linspace(0,1,10)<br>tensor = torch.ones_like(data)<br>print(tensor)                                                        |
| torch.rand_like(tensor)             | 同上                                                     | 与上述类似，创建一个内容随机的张量                     | data = torch.eye(5)<br>x=torch.rand_like(data)<br>print(x)<br><br>                                                                      |
| torch.randint(start,end,type)       | start:int 开始值<br>end: int 结束值<br>type:tuple 形状<br><br> | 创建元素值范围在\[start,end)之间的整数张亮，形状为type   | data = torch.randint(0,20,(3,3))<br>print(data)                                                                                         |
| torch.normal(avg,std,type)          | avg:均值<br>std:方差<br>type:形状<br><br>                    | 创建服从标准正态分布的张量、                        | data = torch.normal(125.32,10.33,(5,8))<br>print(data)<br>                                                                              |
| torch.bernoulli(tensor,probability) | tensor：参考张量<br>probability:概率<br>                      | 从tensor中创建一个概率为probability的服从伯努利分布的张量 | data = torch.bernoulli(torch.rand(5,5),0.2)<br>print(data)<br>                                                                          |
| torch.bartlett_window(length)       | length:长度                                              | 创建一个长度为Length的Bartlett窗口张亮，通常用于信号处理。  | data = torch.bartlett_window(10)<br>print(data)<br>                                                                                     |
| torch.hamming_window(length)        | length：长度                                              | 创建一个长度为10的Hamming窗口张量，常用于信号处理。        | data = torch.hamming_window(5)<br>print(data)<br>                                                                                       |
| torch.hann_window(length)           | 同上                                                     | 创建一个hann窗口张量，常用于信号处理。                 | data = torch.hann_window(20)<br>print(data)<br>                                                                                         |


#### 2.针对张量的属性和方法

张量实际上是Torch中的一种自定义数据类型对象，因此有针对张量的一系列属性和方法。

张量的常见属性：

| 属性            | 说明                                     | 示例代码                                                                                                                                                                                    | 输出                    |
| ------------- | -------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------- |
| tensor.shape  | 用于获取张量的形状                              | torch.Size([3, 4])                                                                                                                                                                      | /                     |
| tensor.size   | 同上                                     | torch.Size([5,6])                                                                                                                                                                       | /                     |
| tensor.dtype  | 用于获取张量的数据类型，在一个张量中，所有的元素具有同一个数据类型      | data = torch.randint(0,20,(3,3))  <br>print(data.dtype)                                                                                                                                 | torch.int64           |
| tensor.device | 获取张量所在的设备                              | tensor = torch.rand(3, 4)  <br>print(tensor.device)  # 输出: cpu  <br>  <br>if torch.cuda.is_available():  <br>    tensor = tensor.to('cuda')  <br>    print(tensor.device)  # 输出: cuda:0 | cpu                   |
| requires_grad | 张量是否需要计算梯度<br>                         | data = torch.rand(5,8,requires_grad=True)  <br>print(data.requires_grad)  <br>a = torch.rand_like(data)  <br>print(a.requires_grad)<br><br>                                             | True<br>False<br>     |
| is_leaf       | 表示张量是否是计算图中的叶子节点。只有叶子节点可以计算梯度。         | x = torch.rand(3, 4, requires_grad=True) <br>y = x + 2 print(x.is_leaf) # 输出: True print(y.is_leaf) # 输出: False                                                                         | True<br>False<br>     |
| grad          | 储存张量的梯度，只在`requires_grad=True` 的情况下有效。 | x = torch.tensor(3.33, requires_grad=True)  <br>y = x ** 2 y.backward()  <br>print(x.grad)<br>                                                                                          | tensor(6.6600)        |
| is_cuda       | 表示张量是否储存在GPU上                          | tensor = torch.rand(3, 4)  <br>print(tensor.is_cuda)  # 输出: False  <br>i·f torch.cuda.is_available():  <br>    tensor = tensor.to('cuda')  <br>    print(tensor.is_cuda)  # 输出: True    | False                 |
| T             | 张量的转置，适用于二维向量。                         | tensor = torch.randint(0,100,(10,12))<br>print(tensor.T)<br>                                                                                                                            | ...                   |
| data          | 访问张量的数据，返回一个不需要梯度的张量                   | tensor = torch.rand(3, 4, requires_grad=True) print(tensor.data)                                                                                                                        | ...                   |
| names         | 张量的命名维度（适用于命名张量）。                      | tensor = torch.rand(3, 4, names=('batch', 'features')) print(tensor.names)                                                                                                              | ('batch', 'features') |

同样的，作为一个对象，张量不仅具有丰富的属性，还具有可供开发者调用的方法：

| 方法名                         | 参数                                      | 说明                                                                                 | 示例                                                                                                                                                                                                                     | 输出                                                                                                                                                                                                                                          |
| --------------------------- | --------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| numel()                     | None                                    | 获取张量的元素总数                                                                          | data = torch.rand(5,3)<br>print(data.numel())<br>                                                                                                                                                                      | 15                                                                                                                                                                                                                                          |
| view(row,col)               | row:变形后的行<br>col:变形后后的列                 | 改变张量的形状                                                                            | tensor = torch.rand(2, 3)  <br>reshaped_tensor = tensor.view(3, 2)  <br>print(reshaped_tensor)                                                                                                                         | ...                                                                                                                                                                                                                                         |
| reshape(row,col)            | 同上                                      | 与view类似，但是更加灵活和常用。                                                                 | tensor = torch.rand(2, 3) reshaped_tensor = tensor.reshape(3, 2) print(reshaped_tensor)                                                                                                                                | ...                                                                                                                                                                                                                                         |
| transpose(dim0,dim1)        | dim0:第一个参与交换的维度<br>dim1：第二个参与交换的维度。<br> | 等同于torch.transpose(input,dim0,dim1)<br>交换张量的两个维度，例如对于一个二维张量（矩阵）进行交换就是将矩阵的行列交换。<br> | tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])  <br>print("Original Tensor:")  <br>print(tensor)  <br>  <br>transposed_tensor = tensor.transpose(0, 1)  <br>print("\nTransposed Tensor:")  <br>print(transposed_tensor) | Original Tensor:<br>tensor([[1, 2, 3],<br>        [4, 5, 6]])<br><br>Transposed Tensor:<br>tensor([[1, 4],<br>        [2, 5],<br>        [3, 6]])                                                                                           |
| permute(dim0,dim1,dim2...)  | dimx:要排列的维度                             | 将张量的维度按指定顺序排列。                                                                     | tensor = torch.rand(2, 3, 4) permuted_tensor = tensor.permute(2, 0, 1) print(permuted_tensor)                                                                                                                          | ...                                                                                                                                                                                                                                         |
| squeeze()                   | None                                    | 移除长度为1的维度                                                                          | tensor = torch.rand(1, 3, 1, 4)  <br>print(tensor.squeeze().shape)                                                                                                                                                     | torch.Size([3, 4])                                                                                                                                                                                                                          |
| unsqueeze(index)            | index：位置                                | 在指定位置添加一个维度                                                                        | tensor = torch.rand(2,5)<br>print(tensor.unsqueeze(2).shape)<br>                                                                                                                                                       | torch.Size([2, 5, 1])                                                                                                                                                                                                                       |
| add(n)/sub(n)/mul(n)/div(n) | n：被运算数                                  | 元素级加减乘除法，即针对每一个元素的操作。                                                              | tensor = torch.randint(1,5,(2,2))<br>print(tensor)<br>print(tensor.add(1))<br>print(tensor.sub(2))<br>print(tensor.mul(0.1))<br>print(tensor.div(3))<br><br>                                                           | tensor([[1, 2],<br>        [4, 2]])<br>tensor([[2, 3],<br>        [5, 3]])<br>tensor([[-1,  0],<br>        [ 2,  0]])<br>tensor([[0.1000, 0.2000],<br>        [0.4000, 0.2000]])<br>tensor([[0.3333, 0.6667],<br>        [1.3333, 0.6667]]) |
| matmul(tensor)              | tensor：另一个二维张量                          | 矩阵乘法                                                                               | tensor = torch.rand(3,3)  <br>tensor1 = torch.rand(3,4)  <br>print(tensor.matmul(tensor1))<br><br><br>                                                                                                                 | ...                                                                                                                                                                                                                                         |
| pow(n)                      | n:n次幂                                   | 元素级幂运算                                                                             | tensor = torch.tensor([16,9,25,81])  <br>print(tensor.pow(2))                                                                                                                                                          | tensor([ 256,   81,  625, 6561])                                                                                                                                                                                                            |
| exp()                       | None                                    | 元素级指数运算。                                                                           | ..                                                                                                                                                                                                                     | ...                                                                                                                                                                                                                                         |
| **sum()**                   | None                                    | 张量求和，返回包含一个元素的标量。                                                                  | data = torch.normal(1.25,2.5,(3,4))  <br>print(data)  <br>print(data.sum())                                                                                                                                            | tensor([[ 1.0224,  2.0065,  1.6491,  1.1283],<br>        [ 2.4744,  0.4406,  1.8718, -0.2045],<br>        [ 0.3385,  0.2063, -2.4550, -1.4317]])<br>tensor(7.0468)                                                                          |
| mean()                      | None                                    | 计算张量均值，返回一个标量型张量                                                                   | tensor = torch.rand(2, 3)<br>result = tensor.mean() print(result)                                                                                                                                                      | tensor(0.5320)                                                                                                                                                                                                                              |
| max()                       | None                                    | 返回张量中的最大值                                                                          | tensor = torch.tensor([1,5,3,0,11])<br>print(tensor.max())<br>                                                                                                                                                         | tensor(11)                                                                                                                                                                                                                                  |
| min()                       | None                                    | 返回张量中最小值                                                                           | ...                                                                                                                                                                                                                    | ...                                                                                                                                                                                                                                         |
| argmax()/argmin()           | None                                    | 返回张量中最大、最小值的索引位置。                                                                  |                                                                                                                                                                                                                        |                                                                                                                                                                                                                                             |
| clone()                     | None                                    | 克隆张量                                                                               |                                                                                                                                                                                                                        |                                                                                                                                                                                                                                             |
| detach()                    | None                                    | 返回新的张量，从当前计算图中分离出来。                                                                | tensor = torch.rand(2, 3, requires_grad=True) detached_tensor = tensor.detach() print(detached_tensor)                                                                                                                 |                                                                                                                                                                                                                                             |
| to(param)                   | param:设备或者数据类型                          | 将张量移动到某个设备，或者转换数据类型。<br>注意：通过to方法转换数据类型返回新的张量而不是修改原张量类型。<br>                       | data = torch.randint(0,12,(2,3))  <br>data = data.to(torch.float32)  <br>print(data.dtype)                                                                                                                             | torch.float32                                                                                                                                                                                                                               |
| cpu()/cuda()                | None                                    | 将张量移动到CPU或者GPU                                                                     |                                                                                                                                                                                                                        |                                                                                                                                                                                                                                             |
| numpy()                     | None                                    | 将张量转为Numpy数组                                                                       |                                                                                                                                                                                                                        |                                                                                                                                                                                                                                             |
| item()                      | None                                    | 将单元素张量转为Python标量数据类型。                                                              |                                                                                                                                                                                                                        |                                                                                                                                                                                                                                             |

### 张量的运算

如上述关于张量的一系列方法，张量的元素级运算也可以通过标准的Python运算符实现。
这里底层的实现是Torch在Tensor类中重写了`__add__()`等用于运算的方法：
![小杜的个人图床](http://src.xiaodu0.com/2024/07/20/9b91a547a3b3e0f96529771aa43d9715.png)
在使用中不必关注这些。

#### 标准算数运算

包含加减乘除和幂运算都是受支持的元素级标准运算符。

Example：
```python
tensor = torch.tensor([25,16,9,49,81,36])  
print(tensor + 10)  
print(tensor - 5)  
print(tensor * 1.5)  
print(tensor / 1.5)  
print(tensor ** 0.5)
```

Output:
```python
tensor([35, 26, 19, 59, 91, 46])
tensor([20, 11,  4, 44, 76, 31])
tensor([ 37.5000,  24.0000,  13.5000,  73.5000, 121.5000,  54.0000])
tensor([16.6667, 10.6667,  6.0000, 32.6667, 54.0000, 24.0000])
tensor([5., 4., 3., 7., 9., 6.])
```

#### 其他运算

1. **张量组合**
我们还可以通过.cat()方法将两个张量结合起来。

语法：
```python
torch.cat((X,Y),dim=z)
```
其中：
- X：要被合并的第一个张量
- Y：要被合并的第二个张量
- z：在哪个维度进行合并，例如dim=0则在行合并，dim=1则在列合并。


Example:
```python
X = torch.arange(15,dtype=torch.float32).reshape((3,5))  
Y = torch.randint(0,20,(5,5))  
print("tensor X:",X)  
print("\r\ntensor Y:",Y)  
print("\r\n combine with dim 0",torch.cat((X,Y),dim=0))  
print("\r\n combine with dim 1",torch.cat((X,Y),dim=1))
```

Output:
```python
tensor X: tensor([[ 0.,  1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.,  9.],
        [10., 11., 12., 13., 14.]])

tensor Y: tensor([[11, 14,  9,  7,  6],
        [ 7,  4,  1,  2,  9],
        [ 2, 14,  6,  8, 12]])

 combine with dim 0 tensor([[ 0.,  1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.,  9.],
        [10., 11., 12., 13., 14.],
        [11., 14.,  9.,  7.,  6.],
        [ 7.,  4.,  1.,  2.,  9.],
        [ 2., 14.,  6.,  8., 12.]])

 combine with dim 1 tensor([[ 0.,  1.,  2.,  3.,  4., 11., 14.,  9.,  7.,  6.],
        [ 5.,  6.,  7.,  8.,  9.,  7.,  4.,  1.,  2.,  9.],
        [10., 11., 12., 13., 14.,  2., 14.,  6.,  8., 12.]])
```

***Notice:根据几次调整shape和dim的值发现，在某一个维度合并，必须确保其他的维度的size是相同的，而当前的维度可以不同。***

例如：
```python
X = torch.arange(32,dtype=torch.float32).reshape((1,2,16))  
Y = torch.randint(0,16,(1,2,8))  
print("tensor X:",X)  
print("\r\ntensor Y:",Y)  
print("\r\n combine with dim 2",torch.cat((X,Y),dim=2))
```

合并在第二维度合并，则第零维和第一维的size必须相同，而第二维可以不同，如果将X,Y的shape调整为：
```python
X = torch.arange(32,dtype=torch.float32).reshape((1,2,16))  
Y = torch.randint(0,16,(1,1,16))  
```
则会抛出如下错误：
```python
RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 2 but got size 1 for tensor number 1 in the list.
```
表明在第二维度的期望尺寸与实际尺寸不符合，期望的尺寸是2，但是实际的尺寸是1。

2. **通过逻辑运算构建二元张量**

当对两个形状相同的张量进行逻辑运算时，产生的结果是由布尔值组成的相同尺寸的张量。

Example：
```python
# 按逻辑产生二元张量  
X = torch.arange(10,dtype=torch.float32).reshape((2,5))  
Y = torch.randint(0,10,(2,5))  
Z = X == Y  
print("tensor X:",X)  
print("\r\ntensor Y:",Y)  
print("X == Y: ",Z)  
print("X != Y: ",~Z)
```

Output:
```python
tensor X: tensor([[0., 1., 2., 3., 4.],
        [5., 6., 7., 8., 9.]])

tensor Y: tensor([[3, 5, 5, 6, 4],
        [6, 7, 4, 2, 9]])
X == Y:  tensor([[False, False, False, False,  True],
        [False, False, False, False,  True]])
X != Y:  tensor([[ True,  True,  True,  True, False],
        [ True,  True,  True,  True, False]])
```

### 广播机制


在PyTorch中，广播机制（broadcasting）允许不同形状的张量在数学运算中自动扩展为相同的形状，从而实现元素级操作。广播机制使得编写代码更加简洁和高效，无需手动调整张量的形状。

tensor的广播机制与Numpy的广播机制类似，主要有以下三个原则：
1. 当两个张量维度不同时，会将较小的维度增加一个维度来匹配较大的维度
2. 如果两个张量在某个维度上不同，但是有一个张量在该维度上的大小是1，则会在这个大小为1的维度上复制以和较大的维度相同。
3. 如果两个张量维度不同，并且大小又都不为1，则无法进行广播，也就是无法进行运算。

总结一下，核心就是：必须有一个维度大小为1才可以进行广播机制来使得两个不同大小的维度进行广播运算。

Example：
```python
X = torch.rand(1,3,dtype=torch.float32)  
Y = torch.normal(1.33,5,(2,1))  
Z = X+Y  
print("tensor X:",X)  
print("\r\ntensor Y:",Y)  
print("X + Y: ",Z)
```
张量X和张量Y在0和1维的大小不同，但是都有一个大小为1的维度，那么此时可以进行广播机制的运算，产生的结果是将X的0维大小复制为2，将Y的1维大小复制为，然后进行运算。

Output：
```python
tensor X: tensor([[0.9192, 0.4674, 0.6165]])

tensor Y: tensor([[-3.2659],
        [ 5.1828]])
X + Y:  tensor([[-2.3467, -2.7986, -2.6494],
        [ 6.1020,  5.6502,  5.7993]])
```

例如：
```python
X = torch.rand(1,3,dtype=torch.float32)  
Y = torch.normal(1.33,5,(2,2))  
Z = X+Y
```
X和Y在第一维的尺寸不同，且都不为1，则无法进行广播运算。

广播运算还有一个值得注意的点是：

**如果对于一个神经网络模型，总是在没有出错的前提下产生不可预料的结果，则需要检测是否因为某个张量的某个维度值为1，导致进行广播运算。**

**因此，我们通常会在模型前向传播之前，调用张量的`squeeze()`归一化方法来移除大小为1的维度来防止广播机制产生意料外的结果。**


### 张量值的修改

与访问张量的值相同，修改张量的值即通过访问张量对应的值，然后修改即可。

Example：修改第二行第四列的值：
```python
X = torch.randint(0,20,(3,5))  
print("tensor X:",X)  
X[1,3]=114514  
print("tensor X after edit:",X)
```

Output:
```python
tensor X: tensor([[ 7,  7, 13, 12,  9],
        [ 6,  4,  4,  1,  3],
        [ 4, 14,  3, 18,  4]])
tensor X after edit: tensor([[     7,      7,     13,     12,      9],
        [     6,      4,      4, 114514,      3],
        [     4,     14,      3,     18,      4]])
```

Example：修改第三行，第1到4列的值：
```python
X = torch.randint(0,20,(3,5))  
print("tensor X:",X)  
X[2,0:4]=torch.tensor([0,1,2,3])  
print("tensor X after edit:",X)
```

Output:
```python
tensor X: tensor([[17, 11,  3,  2, 18],
        [ 9, 15,  5,  0,  7],
        [15, 17,  3,  8,  9]])
tensor X after edit: tensor([[17, 11,  3,  2, 18],
        [ 9, 15,  5,  0,  7],
        [ 0,  1,  2,  3,  9]])
```

Examle: 修改第一行的值全为0：
```python
X = torch.randint(0,20,(3,5))  
print("tensor X:",X)  
X[0,:]=0  
X[0,:]=torch.tensor([0])  
X[0,:]=torch.tensor([0,0,0,0,0])  
# 以上三种方法均可
print("tensor X after edit:",X)
```

### 内存分配相关

由于Python的性能本身就比较有限，而在Torch中又有很多操作会导致内存的重新分配。
大体来说，调用张量类的一系列方法，例如`clone()`、`int()`、`view()`等方法通过一个张量声明另一个张量都会产生新的内存分配，这里由于声明新的变量了，所以无法避免重新分配内存。

这里主要讨论算数运算导致的内存分配。

例如：
```python
X = torch.rand(1,3,dtype=torch.float32)  
before = id(X)  
X = X + 1  
id(X) == before
# False
```

这里很显然，当X对自身进行数学运算时，产生了新的内存分配。

解决这种问题，我们可以使用**原地操作。**

PyTorch 提供了许多原地操作，这些操作会直接修改原始张量而不分配新的内存。原地操作通常以 `_` 结尾，例如 `add_()`, `sub_()`, `mul_()`, `div_()` 等。

Example:
```python
X = torch.rand(1,3,dtype=torch.float32)  
before = id(X)  
print("tensor X:",X)  
X.add_(1)  
print("tensor X:",X)  
id(X) == before
```

Output:
```python
tensor X: tensor([[0.2325, 0.6518, 0.9731]])
tensor X: tensor([[1.2325, 1.6518, 1.9731]])
True
```

同时也可以预先分配内存并重用，在计算之前预先分配好内存，并在计算过程中重用这些内存，而不是每次操作都分配新的内存。

Example：
```python
a = torch.rand(2, 3)  
result = torch.empty(2, 3)  # 预先分配内存  
before = id(result)  
torch.add(a, a, out=result)  # 重用预分配的内存  
print(result)  
id(result) == before
```

Output:
```python
tensor([[0.5254, 1.9610, 1.6119],
        [0.2004, 0.8637, 1.8431]])
True
```

**即在算法运算方法中指定out参数为预先分配的变量。**

